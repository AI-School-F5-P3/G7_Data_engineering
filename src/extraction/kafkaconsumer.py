from confluent_kafka import Consumer, Producer  # Cliente Kafka para consumo de mensajes
from confluent_kafka.admin import  AdminClient        # Cliente admin para gestionar t√≥picos
from src.transformation.datatransformer import process_and_group_data  # Procesamiento de datos
import json
from src.logger import logger                       # Logger personalizado para el sistema
import os


class KafkaConsumer:
    def __init__(self, bootstrap_servers, group_id, redis_loader, mongo_loader, sql_loader):
        """
        Constructor del consumidor Kafka que inicializa las conexiones y loaders.
        
        Args:
            bootstrap_servers (str): Direcci√≥n del servidor Kafka
            group_id (str): ID del grupo de consumidores
            redis_loader (RedisLoader): Gestor del buffer temporal
            mongo_loader (MongoDBLoader): Gestor de almacenamiento en MongoDB
            sql_loader (SQLloader): Gestor de almacenamiento en PostgreSQL
        """
        # Configuraci√≥n b√°sica del consumidor Kafka
        kafka_host = os.getenv('KAFKA_HOST', 'kafka')  # Por defecto usar 'kafka' como hostname
        kafka_port = os.getenv('KAFKA_PORT', '29092')
        self.bootstrap_servers = f"{kafka_host}:{kafka_port}"

        self.config = {
            'bootstrap.servers': self.bootstrap_servers,
            'group.id': 'mi-grupo',
            'auto.offset.reset': 'earliest'
        }

        self.consumer = Consumer(self.config)
        self.producer = Producer({'bootstrap.servers': self.bootstrap_servers})
        self.admin_client = AdminClient({'bootstrap.servers': self.bootstrap_servers})
        try:
            # Inicializar el consumidor y el cliente admin
            self.consumer = Consumer(self.config)
            self.admin_client = AdminClient({'bootstrap.servers': bootstrap_servers})
            logger.info(f"‚úÖ Conexi√≥n exitosa a Kafka en {bootstrap_servers}")
            print(f"‚úÖ Conexi√≥n exitosa a Kafka en {bootstrap_servers}")
        except Exception as e:
            logger.error(f"‚ùå Error al conectar con Kafka: {e}")
            print(f"‚ùå Error al conectar con Kafka: {e}")
            raise e

        # Almacenar referencias a los loaders
        self.redis_loader = redis_loader      # Buffer temporal
        self.mongo_loader = mongo_loader      # Almacenamiento NoSQL
        self.sql_loader = sql_loader          # Almacenamiento SQL
        self.message_count = 0                # Contador de mensajes procesados

    def start_consuming(self):
        """
        Inicia el proceso de consumo continuo de mensajes.
        Este m√©todo es el punto principal de procesamiento.
        """
        try:
            # Obtener lista de t√≥picos disponibles
            metadata = self.admin_client.list_topics(timeout=10)
            available_topics = list(metadata.topics.keys())
            logger.info(f"üìã T√≥picos disponibles: {available_topics}")
            print(f"üìã T√≥picos disponibles: {available_topics}")
            
            # Verificar si hay t√≥picos disponibles
            if not available_topics:
                logger.error("‚ùå No hay t√≥picos disponibles")
                print("‚ùå No hay t√≥picos disponibles")
                return

            # Suscribirse al primer t√≥pico disponible
            primer_topico = available_topics[0]
            self.consumer.subscribe([primer_topico])
            logger.info(f"‚úÖ Suscrito al t√≥pico: {primer_topico}")
            print(f"‚úÖ Suscrito al t√≥pico: {primer_topico}")

            # Bucle principal de consumo
            while True:
                # Intentar obtener un mensaje (timeout 1 segundo)
                msg = self.consumer.poll(1.0)
                
                # Si no hay mensaje, continuar al siguiente ciclo
                if msg is None:
                    continue
                    
                # Manejar errores de Kafka
                if msg.error():
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        logger.warning("‚ö†Ô∏è Fin de la partici√≥n")
                        print("‚ö†Ô∏è Fin de la partici√≥n")
                    else:
                        logger.error(f"‚ùå Error Kafka: {msg.error()}")
                        print(f"‚ùå Error Kafka: {msg.error()}")
                    continue

                # Incrementar contador de mensajes y logging peri√≥dico
                self.message_count += 1
                if self.message_count % 1000 == 0:
                    logger.info(f"üì® Mensajes procesados: {self.message_count}")
                    print(f"üì® Mensajes procesados: {self.message_count}")

                try:
                    # Procesar el mensaje recibido
                    raw_message = msg.value().decode('utf-8')  # Decodificar mensaje
                    transformed_data = process_and_group_data(raw_message)  # Transformar datos

                    # Si no hay error en la transformaci√≥n
                    if "error" not in transformed_data:
                        # A√±adir al buffer de Redis
                        buffer_full = self.redis_loader.add_to_buffer(transformed_data)
                        
                        # Si el buffer est√° lleno, procesar el lote
                        if buffer_full:
                            logger.info("üîÑ Buffer lleno - Iniciando procesamiento del batch")
                            print("üîÑ Buffer lleno - Iniciando procesamiento del batch")
                            self.process_batch()
                    else:
                        logger.warning(f"‚ö†Ô∏è Mensaje inv√°lido: {transformed_data['error']}")
                        print(f"‚ö†Ô∏è Mensaje inv√°lido: {transformed_data['error']}")

                except Exception as e:
                    logger.error(f"‚ùå Error al procesar mensaje: {e}")
                    print(f"‚ùå Error al procesar mensaje: {e}")
                    logger.error(f"Mensaje que caus√≥ el error: {raw_message}")

        except KeyboardInterrupt:
            logger.info("üëã Deteniendo el consumidor por interrupci√≥n del usuario")
            print("üëã Deteniendo el consumidor por interrupci√≥n del usuario")
        finally:
            self.cleanup()

    def process_batch(self):
        """
        Procesa un lote completo de mensajes desde el buffer Redis.
        Los datos se guardan en MongoDB y PostgreSQL.
        """
        try:
            # Obtener datos del buffer
            batch_data = self.redis_loader.get_buffer_batch()
            
            # Verificar si hay datos para procesar
            if not batch_data:
                logger.warning("‚ö†Ô∏è No hay datos para procesar en el batch")
                print("‚ö†Ô∏è No hay datos para procesar en el batch")
                return

            logger.info(f"üì¶ Procesando batch de {len(batch_data)} mensajes")
            print(f"üì¶ Procesando batch de {len(batch_data)} mensajes")
            
            # Guardar datos en MongoDB
            self.mongo_loader.load_to_mongodb(batch_data)
            logger.info(f"‚úÖ Datos guardados en MongoDB")
            
            # Guardar datos en PostgreSQL
            self.sql_loader.load_to_sql(batch_data)
            logger.info(f"‚úÖ Datos guardados en PostgreSQL")
            
            logger.info("‚úÖ Batch procesado exitosamente")
            print("‚úÖ Batch procesado exitosamente")
            
        except Exception as e:
            logger.error(f"‚ùå Error al procesar el batch: {e}")
            print(f"‚ùå Error al procesar el batch: {e}")
            raise e

    def cleanup(self):
        """
        Realiza la limpieza final antes de cerrar el consumidor.
        Procesa mensajes pendientes y cierra conexiones.
        """
        try:
            logger.info("üßπ Iniciando limpieza...")
            print("üßπ Iniciando limpieza...")
            
            # Procesar mensajes finales pendientes
            logger.info("üîÑ Procesando mensajes finales...")
            print("üîÑ Procesando mensajes finales...")
            self.process_batch()
            
            # Cerrar todas las conexiones
            self.consumer.close()
            self.redis_loader.close()
            self.mongo_loader.close()
            
            logger.info("‚úÖ Limpieza completada exitosamente")
            print("‚úÖ Limpieza completada exitosamente")
        except Exception as e:
            logger.error(f"‚ùå Error durante la limpieza: {e}")
            print(f"‚ùå Error durante la limpieza: {e}")