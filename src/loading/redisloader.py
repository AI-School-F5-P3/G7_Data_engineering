import redis
import json
from logger import logger

class RedisLoader:
    def __init__(self, host='localhost', port=6379, db=0, buffer_size=1000):
        """
        Inicializa la conexi√≥n con Redis con optimizaciones de rendimiento.
        """
        try:
            # Configuraci√≥n del pool de conexiones
            self.redis_pool = redis.ConnectionPool(
                host=host,
                port=port,
                db=db,
                decode_responses=True,
                max_connections=10  # Ajustar seg√∫n necesidades
            )
            self.redis_client = redis.Redis(connection_pool=self.redis_pool)
            # Verificar conexi√≥n
            self.redis_client.ping()
            logger.info(f"‚úÖ Conexi√≥n exitosa a Redis en {host}:{port}")
            print(f"‚úÖ Conexi√≥n exitosa a Redis en {host}:{port}")
        except Exception as e:
            logger.error(f"‚ùå Error al conectar con Redis: {e}")
            print(f"‚ùå Error al conectar con Redis: {e}")
            raise e
            
        self.buffer_size = buffer_size
        self.processing_list = "processing_queue"
        self.buffer_count_key = "buffer_count"
        
        # Usar pipelines para limpiar datos anteriores
        with self.redis_client.pipeline() as pipe:
            pipe.delete(self.processing_list)
            pipe.set(self.buffer_count_key, 0)
            pipe.execute()
            
        logger.info(f"Buffer Redis inicializado - Tama√±o m√°ximo: {buffer_size}")
        print(f"Buffer Redis inicializado - Tama√±o m√°ximo: {buffer_size}")

    def add_to_buffer(self, data):
        """
        A√±ade datos al buffer de Redis de manera optimizada usando pipeline
        y minimizando las operaciones de red.
        """
        try:
            with self.redis_client.pipeline() as pipe:
                # Preparar datos en formato JSON
                json_data = json.dumps(data)
                
                # Ejecutar operaciones en batch
                pipe.multi()
                pipe.rpush(self.processing_list, json_data)
                pipe.incr(self.buffer_count_key)
                pipe.get(self.buffer_count_key)
                results = pipe.execute()
                
                # El √∫ltimo resultado es el conteo actual
                buffer_size = int(results[-1])
                
                if buffer_size % 100 == 0:  # Mostrar cada 100 mensajes
                    logger.info(f"üì• Buffer Redis: {buffer_size}/{self.buffer_size}")
                    print(f"üì• Buffer Redis: {buffer_size}/{self.buffer_size}")
                
                return buffer_size >= self.buffer_size
                
        except redis.RedisError as e:
            logger.error(f"‚ùå Error de Redis al a√±adir datos: {e}")
            print(f"‚ùå Error de Redis al a√±adir datos: {e}")
            raise e
        except Exception as e:
            logger.error(f"‚ùå Error general al a√±adir datos: {e}")
            print(f"‚ùå Error general al a√±adir datos: {e}")
            logger.error(f"Datos que causaron el error: {data}")
            raise e

    def get_buffer_batch(self):
        """
        Obtiene un lote de datos del buffer de manera optimizada usando
        pipeline y minimizando las operaciones de lectura/escritura.
        """
        try:
            with self.redis_client.pipeline() as pipe:
                # Ejecutar todas las operaciones en una sola transacci√≥n
                pipe.multi()
                pipe.lrange(self.processing_list, 0, -1)
                pipe.delete(self.processing_list)
                pipe.set(self.buffer_count_key, 0)
                results = pipe.execute()
                
                batch_data = results[0]  # Resultado de lrange
                
                if batch_data:
                    # Procesar datos en batch para mejor rendimiento
                    processed_batch = [json.loads(item) for item in batch_data]
                    batch_size = len(processed_batch)
                    logger.info(f"üì§ Batch obtenido de Redis - {batch_size} elementos")
                    print(f"üì§ Batch obtenido de Redis - {batch_size} elementos")
                    return processed_batch
                
                logger.warning("‚ö†Ô∏è No se encontraron datos en el buffer de Redis")
                print("‚ö†Ô∏è No se encontraron datos en el buffer de Redis")
                return []
                
        except redis.RedisError as e:
            logger.error(f"‚ùå Error de Redis al obtener batch: {e}")
            print(f"‚ùå Error de Redis al obtener batch: {e}")
            raise e
        except Exception as e:
            logger.error(f"‚ùå Error general al obtener batch: {e}")
            print(f"‚ùå Error general al obtener batch: {e}")
            raise e

    def close(self):
        """
        Cierra la conexi√≥n con Redis de manera segura.
        """
        try:
            # Limpiar datos antes de cerrar
            with self.redis_client.pipeline() as pipe:
                pipe.delete(self.processing_list)
                pipe.delete(self.buffer_count_key)
                pipe.execute()
            
            # Cerrar el pool de conexiones
            self.redis_pool.disconnect()
            logger.info("‚úÖ Conexi√≥n a Redis cerrada correctamente")
            print("‚úÖ Conexi√≥n a Redis cerrada correctamente")
        except Exception as e:
            logger.error(f"‚ùå Error al cerrar la conexi√≥n con Redis: {e}")
            print(f"‚ùå Error al cerrar la conexi√≥n con Redis: {e}")